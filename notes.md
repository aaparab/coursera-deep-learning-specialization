# 1. Neural Networks and Deep Learning
---

## My notes:

### Week 1: 

- Scale drives deep learning progress: Increase in labelled data, computational power and better algorithms have made deep learning take off. 

- The slope of the sigmoid function is close to zero for really small and really large input values. This makes relu a better choice, since **gradient descent becomes faster**.[Video reference](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/praGm/why-is-deep-learning-taking-off)

### Week 2: 

- One reason that *mean squared error* is not used in the cost function of logistic regression is that doing so makes the cost function non-convex. [Video reference](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/yWaRd/logistic-regression-cost-function)
